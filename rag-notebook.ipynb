{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts gradio.exe and upload_theme.exe are installed in 'C:\\Users\\ROSHAN GEORGE\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -qU langchain langchain-community\n",
    "!pip install -qU beautifulsoup4\n",
    "!pip install -qU faiss-cpu\n",
    "#!pip install -qU faiss-gpu\n",
    "!pip install -qU chromadb\n",
    "!pip install -qU validators\n",
    "!pip install -qU sentence_transformers typing-extensions==4.8.0 unstructured\n",
    "!pip install -qU gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROSHAN GEORGE\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\ROSHAN GEORGE\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 8a5a387c-921e-4cf5-8e56-abd7180212e4 completed with result: {'message': 'Processed 2 documents from 2 URLs'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import uuid\n",
    "import uvicorn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import gradio as gr\n",
    "from fastapi import FastAPI, Request, Header, HTTPException\n",
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from utils import StatusCodes\n",
    "from utils.version import API_VERSION, SERVICE_NAME\n",
    "from models import Request, LangChainRequestCall, UploadRequestCall, TaskRequest\n",
    "from fastapi import FastAPI, Request, Header\n",
    "import time\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "FAISS_INDEX_PATH = \"./vectorstore/lc-faiss-multi-mpnet-500\"\n",
    "SUPPORTED_METHOD = [\"ask\", \"upload_links\"]\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "task_status = {}\n",
    "task_results = {} \n",
    "    \n",
    "# Helper functions\n",
    "def response_template(request_id: str, \n",
    "                      trace_id: str, \n",
    "                      process_duration: int,\n",
    "                      isResponseImmediate: bool,\n",
    "                      response: dict,\n",
    "                      error_code: dict):\n",
    "    now = datetime.datetime.now().isoformat()\n",
    "    response_data = {\n",
    "        \"requestId\": request_id,\n",
    "        \"traceId\": trace_id,\n",
    "        \"apiVersion\": API_VERSION,\n",
    "        \"service\": SERVICE_NAME,\n",
    "        \"datetime\": now,\n",
    "        \"isResponseImmediate\": isResponseImmediate,\n",
    "        \"processDuration\": process_duration,\n",
    "        \"response\": response,\n",
    "        \"errorCode\": error_code,\n",
    "    }\n",
    "    return response_data\n",
    "\n",
    "# LangChain setup\n",
    "urls = [\"https://langchain-ai.github.io/langgraph/#example\"]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url,\n",
    "        max_depth=9,\n",
    "        extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents(\n",
    "    [doc.page_content for doc in docs], \n",
    "    metadatas=[doc.metadata for doc in docs]\n",
    ")\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "db = FAISS.from_documents(filter_complex_metadata(chunks), embeddings)\n",
    "db.save_local(FAISS_INDEX_PATH)\n",
    "\n",
    "model_id = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"return_full_text\": False\n",
    "    },\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "\n",
    "db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "You are the friendly documentation buddy Solido, who helps novice programmers in using LangChain with simple explanations and examples.\\\n",
    "    Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question :\n",
    "------\n",
    "<ctx>\n",
    "{context}\n",
    "</ctx>\n",
    "------\n",
    "<hs>\n",
    "{history}\n",
    "</hs>\n",
    "------\n",
    "{question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model_id,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"memory\": memory,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "# API routes\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(request: LangChainRequestCall):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        history = [(q, a) for q, a in request.request.payload.history if q or a]\n",
    "        response = qa({\"query\": request.request.payload.query, \"history\": history})\n",
    "        process_duration = int((time.time() - start_time) * 1000)\n",
    "        result = response.get('result', response.get('answer', str(response)))\n",
    "        \n",
    "        return response_template(\n",
    "            request_id=request.requestId,\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=process_duration,\n",
    "            isResponseImmediate=True,\n",
    "            response={\"result\": result},\n",
    "            error_code={}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        process_duration = int((time.time() - start_time) * 1000)\n",
    "        return response_template(\n",
    "            request_id=request.requestId,\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=process_duration,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"code\": \"ERROR\", \"message\": str(e)}\n",
    "        )\n",
    "\n",
    "@app.post(\"/upload_links\")\n",
    "async def upload_links(request: UploadRequestCall):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        links = request.request.payload.urls\n",
    "        global docs, chunks, db, retriever, qa\n",
    "        \n",
    "        docs = []\n",
    "        for url in links:\n",
    "            loader = RecursiveUrlLoader(url=url, max_depth=9, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "            docs.extend(loader.load())\n",
    "        \n",
    "        chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
    "        \n",
    "        db = FAISS.from_documents(filter_complex_metadata(chunks), embeddings)\n",
    "        db.save_local(FAISS_INDEX_PATH)\n",
    "        \n",
    "        retriever = db.as_retriever()\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=model_id,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            verbose=True,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\n",
    "                \"verbose\": True,\n",
    "                \"memory\": memory,\n",
    "                \"prompt\": prompt\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        process_duration = int((time.time() - start_time) * 1000)\n",
    "        return response_template(\n",
    "            request_id=request.requestId,\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=process_duration,\n",
    "            isResponseImmediate=True,\n",
    "            response={\"message\": f\"Processed {len(docs)} documents from {len(links)} URLs\"},\n",
    "            error_code={}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        process_duration = int((time.time() - start_time) * 1000)\n",
    "        return response_template(\n",
    "            request_id=request.requestId,\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=process_duration,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"code\": \"ERROR\", \"message\": str(e)}\n",
    "        )\n",
    "\n",
    "def response_template(request_id: str, trace_id: str, process_duration: int,\n",
    "                      isResponseImmediate: bool, response: dict, error_code: dict):\n",
    "    now = datetime.datetime.now().isoformat()\n",
    "    return {\n",
    "        \"requestId\": request_id,\n",
    "        \"traceId\": trace_id,\n",
    "        \"apiVersion\": \"1.0\",\n",
    "        \"service\": \"LangChainQA\",\n",
    "        \"datetime\": now,\n",
    "        \"isResponseImmediate\": isResponseImmediate,\n",
    "        \"processDuration\": process_duration,\n",
    "        \"response\": response,\n",
    "        \"errorCode\": error_code,\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/call\")\n",
    "async def call_endpoint(request: Request, x_user_id: str = Header(None)):\n",
    "    start_time = time.time()\n",
    "    request_data = await request.json()\n",
    "\n",
    "    if not x_user_id:\n",
    "        return response_template(\n",
    "            request_id=str(uuid.uuid4()),\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=-1,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"status\": StatusCodes.INVALID_REQUEST, \"reason\": \"userToken is invalid\"}\n",
    "        )\n",
    "\n",
    "    request_id = request.headers.get('x-request-id')\n",
    "    if not request_id:\n",
    "        return response_template(\n",
    "            request_id=str(uuid.uuid4()),\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=-1,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"status\": StatusCodes.INVALID_REQUEST, \"reason\": \"requestId is invalid\"}\n",
    "        )\n",
    "\n",
    "    method = request_data.get('method')\n",
    "    if not method:\n",
    "        return response_template(\n",
    "            request_id=request_id,\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=-1,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"status\": StatusCodes.INVALID_REQUEST, \"reason\": \"method is invalid\"}\n",
    "        )\n",
    "    elif method not in SUPPORTED_METHOD:\n",
    "        return response_template(\n",
    "            request_id=request_id,\n",
    "            trace_id=str(uuid.uuid4()),\n",
    "            process_duration=-1,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"status\": StatusCodes.UNSUPPORTED, \"reason\": f\"unsupported method {method}\"}\n",
    "        )\n",
    "\n",
    "    task_id = str(uuid.uuid4())\n",
    "    task_status[task_id] = StatusCodes.PENDING  # Set initial status as PENDING\n",
    "\n",
    "    process_duration = int((time.time() - start_time) * 1000)\n",
    "\n",
    "    # Start a background thread to process the request\n",
    "    threading.Thread(target=process_task, args=(task_id, request_data, x_user_id, request_id)).start()\n",
    "\n",
    "    return response_template(\n",
    "        request_id=request_id,\n",
    "        trace_id=str(uuid.uuid4()),\n",
    "        process_duration=process_duration,\n",
    "        isResponseImmediate=False,\n",
    "        response={\"taskId\": task_id},\n",
    "        error_code={\"status\": StatusCodes.PENDING, \"reason\": \"Task is pending\"}\n",
    "    )\n",
    "\n",
    "# Define your task processing function\n",
    "def process_task(task_id, request_data, user_id, request_id):\n",
    "    start_time = time.time()\n",
    "    task_status[task_id] = StatusCodes.INPROGRESS  # Update status to INPROGRESS\n",
    "\n",
    "    method = request_data.get('method')\n",
    "    payload = request_data.get('payload', {})\n",
    "    \n",
    "    try:\n",
    "        if method == \"ask\":\n",
    "            query = payload.get('query', '')\n",
    "            history = payload.get('history', [])\n",
    "            response = qa({\"query\": query, \"history\": history})\n",
    "            result = response.get('result', response.get('answer', str(response)))\n",
    "            \n",
    "        elif method == \"upload_links\":\n",
    "            urls = payload.get('urls', [])\n",
    "            global docs, chunks, db, retriever\n",
    "            \n",
    "            docs = []\n",
    "            for url in urls:\n",
    "                loader = RecursiveUrlLoader(url=url, max_depth=9, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "                docs.extend(loader.load())\n",
    "            \n",
    "            chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
    "            \n",
    "            db = FAISS.from_documents(filter_complex_metadata(chunks), embeddings)\n",
    "            db.save_local(FAISS_INDEX_PATH)\n",
    "            \n",
    "            retriever = db.as_retriever()\n",
    "            qa = RetrievalQA.from_chain_type(\n",
    "                llm=model_id,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                verbose=True,\n",
    "                return_source_documents=True,\n",
    "                chain_type_kwargs={\n",
    "                    \"verbose\": True,\n",
    "                    \"memory\": memory,\n",
    "                    \"prompt\": prompt\n",
    "                }\n",
    "            )\n",
    "            result = {\"message\": f\"Processed {len(docs)} documents from {len(urls)} URLs\"}\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method\")\n",
    "\n",
    "        task_status[task_id] = StatusCodes.SUCCESS  # Update status to SUCCESS\n",
    "\n",
    "    except Exception as e:\n",
    "        task_status[task_id] = StatusCodes.ERROR  # Update status to ERROR\n",
    "        result = {\"message\": str(e)}\n",
    "\n",
    "    process_duration = int((time.time() - start_time) * 1000)\n",
    "    task_results[task_id] = result\n",
    "    print(f\"Task {task_id} completed with result: {result}\")\n",
    "    \n",
    "\n",
    "    # You might want to store the result somewhere so it can be retrieved later\n",
    "    # For example, you could use a dictionary to store results:\n",
    "    # task_results[task_id] = result\n",
    "\n",
    "\n",
    "    # Optionally, implement a callback function to notify the client of completion\n",
    "    # send_callback(user_id, task_id, request_id, process_duration, result)\n",
    "\n",
    "# Uncomment and implement this function if you need to send a callback notification\n",
    "# def send_callback(user_id, task_id, request_id, process_duration, result):\n",
    "#     callback_message = {\n",
    "#         \"taskId\": task_id,\n",
    "#         \"data\": result,\n",
    "#         \"processDuration\": process_duration,\n",
    "#         \"status\": \"success\"\n",
    "#     }\n",
    "#     # Replace with actual callback URL and send the request\n",
    "#     # requests.post(\"callback_url\", json=callback_message)\n",
    "\n",
    "\n",
    "def response_template(request_id, trace_id, process_duration, isResponseImmediate, response, error_code):\n",
    "    return {\n",
    "        \"request_id\": request_id,\n",
    "        \"trace_id\": trace_id,\n",
    "        \"process_duration\": process_duration,\n",
    "        \"isResponseImmediate\": isResponseImmediate,\n",
    "        \"response\": response,\n",
    "        \"error_code\": error_code\n",
    "    }\n",
    "\n",
    "def success_response(task_id, data, data_type, request_id, trace_id, process_duration):\n",
    "    return response_template(\n",
    "        request_id=request_id,\n",
    "        trace_id=trace_id,\n",
    "        process_duration=process_duration,\n",
    "        isResponseImmediate=True,\n",
    "        response={\n",
    "            \"taskId\": task_id,\n",
    "            \"data\": data,\n",
    "            \"dataType\": data_type\n",
    "        },\n",
    "        error_code={\"status\": StatusCodes.SUCCESS, \"reason\": \"Task completed successfully\"}\n",
    "    )\n",
    "\n",
    "@app.post(\"/result\")\n",
    "async def result(request: Request, task_request: TaskRequest, x_user_id: str = Header(None), x_request_id: str = Header(None)):\n",
    "    start_time = time.time()\n",
    "    trace_id = str(uuid.uuid4())\n",
    "    result_request_id = str(uuid.uuid4())\n",
    "\n",
    "    if x_user_id is None or not x_user_id.strip():\n",
    "        error_code = {\"status\": StatusCodes.ERROR, \"reason\": \"No User ID found in headers\"}\n",
    "        response_data = response_template(result_request_id, trace_id, -1, True, {}, error_code)\n",
    "        raise HTTPException(status_code=400, detail=response_data)\n",
    "\n",
    "    if x_request_id is None or not x_request_id.strip():\n",
    "        error_code = {\"status\": StatusCodes.ERROR, \"reason\": \"No request ID found in headers\"}\n",
    "        response_data = response_template(result_request_id, trace_id, -1, True, {}, error_code)\n",
    "        raise HTTPException(status_code=400, detail=response_data)\n",
    "    \n",
    "    if task_request.taskId is None or not task_request.taskId.strip():\n",
    "        error_code = {\"status\": StatusCodes.ERROR, \"reason\": \"No task ID found in body\"}\n",
    "        response_data = response_template(result_request_id, trace_id, -1, True, {}, error_code)\n",
    "        raise HTTPException(status_code=400, detail=response_data)\n",
    "\n",
    "    status_code = task_status.get(task_request.taskId, StatusCodes.ERROR)\n",
    "    if status_code == StatusCodes.SUCCESS:\n",
    "        # Retrieve the actual result\n",
    "        result = task_results.get(task_request.taskId, {\"message\": \"No result found\"})\n",
    "        response_data = success_response(\n",
    "            task_request.taskId, result, \"RESULT_DATA\", x_request_id, trace_id, int((time.time() - start_time) * 1000)\n",
    "        )\n",
    "    elif status_code == StatusCodes.PENDING:\n",
    "        response_data = response_template(\n",
    "            request_id=result_request_id,\n",
    "            trace_id=trace_id,\n",
    "            process_duration=-1,\n",
    "            isResponseImmediate=False,\n",
    "            response={\"taskId\": task_request.taskId},\n",
    "            error_code={\"status\": StatusCodes.PENDING, \"reason\": \"Task is still pending\"}\n",
    "        )\n",
    "    elif status_code == StatusCodes.INPROGRESS:\n",
    "        response_data = response_template(\n",
    "        request_id=result_request_id,\n",
    "        trace_id=trace_id,\n",
    "        process_duration=-1,\n",
    "        isResponseImmediate=False,\n",
    "        response={\"taskId\": task_request.taskId},\n",
    "        error_code={\"status\": StatusCodes.INPROGRESS, \"reason\": \"Task is processing\"}\n",
    "    )\n",
    "    else:\n",
    "        response_data = response_template(\n",
    "            request_id=result_request_id,\n",
    "            trace_id=trace_id,\n",
    "            process_duration=-1,\n",
    "            isResponseImmediate=True,\n",
    "            response={},\n",
    "            error_code={\"status\": StatusCodes.ERROR, \"reason\": \"Task failed or error occurred\"}\n",
    "        )\n",
    "\n",
    "    return response_data\n",
    "\n",
    "# Gradio interface\n",
    "def infer(question, history):\n",
    "    formatted_history = [(str(q) if q is not None else \"\", str(a) if a is not None else \"\") for q, a in history]\n",
    "    \n",
    "    response = requests.post(\"http://localhost:8000/ask\", \n",
    "                             json={\"userToken\": \"dummy_token\",\n",
    "                                   \"requestId\": \"dummy_id\",\n",
    "                                   \"request\": {\n",
    "                                       \"method\": \"ask\",\n",
    "                                       \"payload\": {\"query\": question, \"history\": formatted_history}\n",
    "                                   }})\n",
    "    return response.json()\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, \"\"\n",
    "\n",
    "def bot(history):\n",
    "    response = infer(history[-1][0], history[:-1])\n",
    "    if 'response' in response and 'result' in response['response']:\n",
    "        history[-1][1] = response['response']['result']\n",
    "    else:\n",
    "        history[-1][1] = \"Sorry, I couldn't generate a response.\"\n",
    "    return history\n",
    "\n",
    "def upload_links_ui(links):\n",
    "    urls = [url.strip() for url in links.split(',')]\n",
    "    response = requests.post(\"http://localhost:8000/upload_links\", \n",
    "                             json={\n",
    "                                 \"userToken\": \"dummy_token\",\n",
    "                                 \"requestId\": \"dummy_id\",\n",
    "                                 \"request\": {\n",
    "                                     \"method\": \"upload_links\",\n",
    "                                     \"payload\": {\n",
    "                                         \"urls\": urls\n",
    "                                     }\n",
    "                                 }\n",
    "                             })\n",
    "    response_data = response.json()\n",
    "    \n",
    "    if 'response' in response_data and 'message' in response_data['response']:\n",
    "        return response_data['response']['message']\n",
    "    elif 'errorCode' in response_data and 'message' in response_data['errorCode']:\n",
    "        return f\"Error: {response_data['errorCode']['message']}\"\n",
    "    else:\n",
    "        return str(response_data)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column(elem_id=\"col-container\"):\n",
    "        chatbot = gr.Chatbot([], elem_id=\"chatbot\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "        with gr.Row():\n",
    "            question = gr.Textbox(label=\"Question\", placeholder=\"Type your question and hit Enter \")\n",
    "\n",
    "        with gr.Row():\n",
    "                    links_input = gr.Textbox(label=\"Upload Links\", placeholder=\"Enter comma-separated URLs\")\n",
    "                    upload_button = gr.Button(\"Upload\")\n",
    "                    upload_status = gr.Textbox(label=\"Upload Status\")\n",
    "\n",
    "        question.submit(add_text, [chatbot, question], [chatbot, question], queue=False).then(\n",
    "            bot, chatbot, chatbot\n",
    "        )\n",
    "\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "        \n",
    "        upload_button.click(upload_links_ui, inputs=[links_input], outputs=[upload_status])\n",
    "\n",
    "# Run both FastAPI and Gradio\n",
    "if __name__ == \"__main__\":\n",
    "    import threading\n",
    "    \n",
    "    def run_fastapi():\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    \n",
    "    def run_gradio():\n",
    "        demo.launch(share=False)\n",
    "    \n",
    "    # Start FastAPI in a separate thread\n",
    "    fastapi_thread = threading.Thread(target=run_fastapi)\n",
    "    fastapi_thread.start()\n",
    "    \n",
    "    # Run Gradio in the main thread\n",
    "    run_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
